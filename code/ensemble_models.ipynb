{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e4da0b",
   "metadata": {},
   "source": [
    "# DengAI: Ensemble Model (XGBoost + SVM + CNN → Random Forest)\n",
    "\n",
    "**Competition:** [DengAI: Predicting Disease Spread](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/)\n",
    "\n",
    "## Ensemble Architecture\n",
    "\n",
    "This notebook implements a **stacking ensemble** approach:\n",
    "\n",
    "### Base Models:\n",
    "1. **XGBoost Regressor** - Gradient boosting tree model\n",
    "2. **SVM (SVR)** - Support Vector Regression with RBF kernel\n",
    "3. **1D CNN** - Convolutional Neural Network for time series\n",
    "\n",
    "### Meta-Learner (Ensembler):\n",
    "- **Random Forest Regressor** - Combines base model predictions\n",
    "\n",
    "### Key Features:\n",
    "- No oversampling (clean time series split)\n",
    "- StandardScaler for feature normalization\n",
    "- Grid search for hyperparameter tuning\n",
    "- Separate models for San Juan and Iquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "WORKING_DIR = 'C:/term_project/'\n",
    "os.chdir(WORKING_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS & ENV VARS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Models\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# TensorFlow/Keras for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "DATA_DIR = os.path.join(WORKING_DIR, 'data')\n",
    "RESULT_DIR = os.path.join(WORKING_DIR, 'results')\n",
    "\n",
    "# Feature Engineering Parameters\n",
    "LAG_WEEKS = [4, 8, 12, 16, 20, 24, 28, 32, 36, 41, 52]\n",
    "ROLLING_WEEKS = [4, 8, 12, 16, 20, 24, 28, 32, 36, 41, 52]\n",
    "DELTA_WEEKS = [1, 2, 3, 4, 8, 12, 16, 20, 24, 28, 32, 36, 41, 52]\n",
    "\n",
    "# Split Ratios (No Oversampling)\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "# XGBoost Parameter Grid\n",
    "XGB_PARAM_GRID = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# SVM Parameter Grid\n",
    "SVM_PARAM_GRID = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "    'epsilon': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# CNN Parameters\n",
    "CNN_EPOCHS = 100\n",
    "CNN_BATCH_SIZE = 32\n",
    "\n",
    "# Random Forest (Ensembler) Parameters\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "\n",
    "train_features = pd.read_csv(os.path.join(DATA_DIR, 'dengue_features_train.csv'), index_col=[0, 1, 2])\n",
    "train_labels = pd.read_csv(os.path.join(DATA_DIR, 'dengue_labels_train.csv'), index_col=[0, 1, 2])\n",
    "\n",
    "# Seperate data for San Juan\n",
    "sj_train_features = train_features.loc['sj']\n",
    "sj_train_labels = train_labels.loc['sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features.loc['iq']\n",
    "iq_train_labels = train_labels.loc['iq']\n",
    "\n",
    "print(\"San Juan\")\n",
    "print(\"features: \", sj_train_features.shape)\n",
    "print(\"labels  : \", sj_train_labels.shape)\n",
    "\n",
    "print(\"\\nIquitos\")\n",
    "print(\"features: \", iq_train_features.shape)\n",
    "print(\"labels  : \", iq_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"San Juan Features:\", sj_train_features.columns.tolist())\n",
    "print(\"San Juan Labels:\", sj_train_labels.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32934bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS DATA \n",
    "\n",
    "def preprocess_city_data(features, labels=None):\n",
    "    \"\"\"\n",
    "    Merges features and labels, converts week_start_date to index,\n",
    "    and handles missing values.\n",
    "    \"\"\"\n",
    "    # 1. Merge features and labels if labels are provided\n",
    "    if labels is not None:\n",
    "        df = features.join(labels)\n",
    "    else:\n",
    "        df = features.copy()\n",
    "    \n",
    "    # 2. Reset index to move 'year' and 'weekofyear' from index to columns\n",
    "    # This preserves them as features.\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # 3. Convert week_start_date to datetime\n",
    "    df['week_start_date'] = pd.to_datetime(df['week_start_date'])\n",
    "    \n",
    "    # 4. Set week_start_date as index\n",
    "    df.set_index('week_start_date', inplace=True)\n",
    "    \n",
    "    # 5. Drop 'city' column as it's constant for each model and not numeric\n",
    "    if 'city' in df.columns:\n",
    "        df.drop(columns=['city'], inplace=True)\n",
    "    \n",
    "    # 6. Fill Missing Values (Forward Fill for time series)\n",
    "    df = df.ffill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to San Juan and Iquitos\n",
    "sj_train = preprocess_city_data(sj_train_features, sj_train_labels)\n",
    "iq_train = preprocess_city_data(iq_train_features, iq_train_labels)\n",
    "\n",
    "print(\"San Juan Preprocessed Shape:\", sj_train.shape)\n",
    "print(\"Iquitos Preprocessed Shape:\", iq_train.shape)\n",
    "sj_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c509052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION\n",
    "\n",
    "def create_features(df, lag_weeks=LAG_WEEKS, rolling_weeks=ROLLING_WEEKS, delta_weeks=DELTA_WEEKS):\n",
    "    \"\"\"\n",
    "    Creates lag and rolling window features.\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # List of columns to create features for (exclude labels and non-numeric if any)\n",
    "    # Typically climate variables\n",
    "    feature_cols = [c for c in df.columns if c not in ['total_cases', 'year', 'weekofyear']]\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        # Lag features\n",
    "        for lag in lag_weeks:\n",
    "            new_features.append(\n",
    "                df_eng[col].shift(lag).rename(f'{col}_lag_{lag}')\n",
    "            )\n",
    "\n",
    "        # Delta features\n",
    "        for delta in delta_weeks:\n",
    "            new_features.append(\n",
    "                (df_eng[col] - df_eng[col].shift(delta)).rename(f'{col}_delta_{delta}')\n",
    "            )\n",
    "            \n",
    "        # Rolling features\n",
    "        for window in rolling_weeks:\n",
    "            rolling_col = df_eng[col].rolling(window=window)\n",
    "            \n",
    "            new_features.append(rolling_col.mean().rename(f'{col}_rolling_mean_{window}'))\n",
    "            new_features.append(rolling_col.std().rename(f'{col}_rolling_std_{window}'))\n",
    "            new_features.append(rolling_col.var().rename(f'{col}_rolling_var_{window}'))\n",
    "            new_features.append(rolling_col.min().rename(f'{col}_rolling_min_{window}'))\n",
    "            new_features.append(rolling_col.max().rename(f'{col}_rolling_max_{window}'))\n",
    "            new_features.append(rolling_col.median().rename(f'{col}_rolling_median_{window}'))\n",
    "            new_features.append(rolling_col.skew().rename(f'{col}_rolling_skew_{window}'))\n",
    "            new_features.append(rolling_col.kurt().rename(f'{col}_rolling_kurt_{window}'))\n",
    "\n",
    "        # # Z-Score features\n",
    "        # for window in rolling_weeks:\n",
    "        #     rolling_mean = df_eng[col].rolling(window=window).mean()\n",
    "        #     rolling_std = df_eng[col].rolling(window=window).std()\n",
    "        #     z_score = ((df_eng[col] - rolling_mean) / rolling_std).rename(f'{col}_zscore_{window}')\n",
    "        #     new_features.append(z_score)\n",
    "            \n",
    "    # Concatenate all new features at once to avoid PerformanceWarning (fragmentation)\n",
    "    if new_features:\n",
    "        df_features = pd.concat(new_features, axis=1)\n",
    "        df_eng = pd.concat([df_eng, df_features], axis=1)\n",
    "            \n",
    "    # Drop rows with NaNs created by lag/rolling (or fill them)\n",
    "    # For training, we usually drop the initial rows.\n",
    "    df_eng.dropna(inplace=True)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Create engineered features\n",
    "sj_train_eng = create_features(sj_train)\n",
    "iq_train_eng = create_features(iq_train)\n",
    "\n",
    "print(\"San Juan Engineered Shape:\", sj_train_eng.shape)\n",
    "print(\"Iquitos Engineered Shape:\", iq_train_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SELECTED FEATURES\n",
    "\n",
    "sj_selected_features_path = os.path.join(RESULT_DIR, 'sj_selected_features.json')\n",
    "with open(sj_selected_features_path, 'r') as f:\n",
    "    sj_features_selected = json.load(f)\n",
    "    \n",
    "iq_selected_features_path = os.path.join(RESULT_DIR, 'iq_selected_features.json')\n",
    "with open(iq_selected_features_path, 'r') as f:\n",
    "    iq_features_selected = json.load(f)\n",
    "\n",
    "# Filter engineered data to selected features only\n",
    "sj_train_eng = sj_train_eng[sj_features_selected + ['total_cases']]\n",
    "iq_train_eng = iq_train_eng[iq_features_selected + ['total_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA (NO OVERSAMPLING)\n",
    "\n",
    "def split_data(df, target_col='total_cases', train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Splits data into Train, Validation, and Test sets.\n",
    "    No oversampling - clean time series split.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "    \n",
    "    # Separate X and y\n",
    "    X_train = train.drop(columns=[target_col])\n",
    "    y_train = train[target_col]\n",
    "    \n",
    "    X_val = val.drop(columns=[target_col])\n",
    "    y_val = val[target_col]\n",
    "    \n",
    "    X_test = test.drop(columns=[target_col])\n",
    "    y_test = test[target_col]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "print(\"Splitting San Juan...\")\n",
    "X_train_sj, y_train_sj, X_val_sj, y_val_sj, X_test_sj, y_test_sj = split_data(sj_train_eng)\n",
    "\n",
    "print(\"Splitting Iquitos...\")\n",
    "X_train_iq, y_train_iq, X_val_iq, y_val_iq, X_test_iq, y_test_iq = split_data(iq_train_eng)\n",
    "\n",
    "print(\"\\nSan Juan Shapes (Train/Val/Test):\", X_train_sj.shape, X_val_sj.shape, X_test_sj.shape)\n",
    "print(\"Iquitos Shapes (Train/Val/Test):\", X_train_iq.shape, X_val_iq.shape, X_test_iq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811032a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SPLITS\n",
    "\n",
    "def plot_splits(y_train, y_val, y_test, title):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(y_train.index, y_train, label='Train')\n",
    "    plt.plot(y_val.index, y_val, label='Validation', color='orange')\n",
    "    plt.plot(y_test.index, y_test, label='Test', color='green')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_splits(y_train_sj, y_val_sj, y_test_sj, \"San Juan Data Splits\")\n",
    "plot_splits(y_train_iq, y_val_iq, y_test_iq, \"Iquitos Data Splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE DATA (Required for SVM and CNN)\n",
    "\n",
    "# Create scalers for each city\n",
    "scaler_sj = StandardScaler()\n",
    "scaler_iq = StandardScaler()\n",
    "\n",
    "# Fit on training data only, transform all sets\n",
    "X_train_sj_scaled = scaler_sj.fit_transform(X_train_sj)\n",
    "X_val_sj_scaled = scaler_sj.transform(X_val_sj)\n",
    "X_test_sj_scaled = scaler_sj.transform(X_test_sj)\n",
    "\n",
    "X_train_iq_scaled = scaler_iq.fit_transform(X_train_iq)\n",
    "X_val_iq_scaled = scaler_iq.transform(X_val_iq)\n",
    "X_test_iq_scaled = scaler_iq.transform(X_test_iq)\n",
    "\n",
    "# Convert to DataFrames for XGBoost (keeps column names)\n",
    "X_train_sj_scaled_df = pd.DataFrame(X_train_sj_scaled, index=X_train_sj.index, columns=X_train_sj.columns)\n",
    "X_val_sj_scaled_df = pd.DataFrame(X_val_sj_scaled, index=X_val_sj.index, columns=X_val_sj.columns)\n",
    "X_test_sj_scaled_df = pd.DataFrame(X_test_sj_scaled, index=X_test_sj.index, columns=X_test_sj.columns)\n",
    "\n",
    "X_train_iq_scaled_df = pd.DataFrame(X_train_iq_scaled, index=X_train_iq.index, columns=X_train_iq.columns)\n",
    "X_val_iq_scaled_df = pd.DataFrame(X_val_iq_scaled, index=X_val_iq.index, columns=X_val_iq.columns)\n",
    "X_test_iq_scaled_df = pd.DataFrame(X_test_iq_scaled, index=X_test_iq.index, columns=X_test_iq.columns)\n",
    "\n",
    "print(\"Scaling complete!\")\n",
    "print(\"San Juan scaled shape:\", X_train_sj_scaled.shape)\n",
    "print(\"Iquitos scaled shape:\", X_train_iq_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ee5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL 1: XGBOOST =====\n",
    "\n",
    "def tune_xgboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Performs Grid Search for XGBoost by training on Train and evaluating on Validation set.\n",
    "    \"\"\"\n",
    "    param_grid = list(ParameterGrid(XGB_PARAM_GRID))\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"Tuning XGBoost on {len(param_grid)} combinations...\")\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        current_params = params.copy()\n",
    "        current_params.update({\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        })\n",
    "        \n",
    "        model = xgb.XGBRegressor(**current_params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        preds = np.maximum(preds, 0)\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        \n",
    "        if mae < best_score:\n",
    "            best_score = mae\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best MAE (Validation):\", best_score)\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Tune San Juan XGBoost\n",
    "print(\"=\" * 50)\n",
    "print(\"Tuning San Juan XGBoost Model...\")\n",
    "xgb_model_sj, xgb_params_sj = tune_xgboost(X_train_sj_scaled_df, y_train_sj, X_val_sj_scaled_df, y_val_sj)\n",
    "\n",
    "# Tune Iquitos XGBoost\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Tuning Iquitos XGBoost Model...\")\n",
    "xgb_model_iq, xgb_params_iq = tune_xgboost(X_train_iq_scaled_df, y_train_iq, X_val_iq_scaled_df, y_val_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c30f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL 2: SVM (Support Vector Regression) =====\n",
    "\n",
    "def tune_svm(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Performs Grid Search for SVM by training on Train and evaluating on Validation set.\n",
    "    SVM requires scaled data.\n",
    "    \"\"\"\n",
    "    param_grid = list(ParameterGrid(SVM_PARAM_GRID))\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"Tuning SVM on {len(param_grid)} combinations...\")\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        current_params = params.copy()\n",
    "        current_params['kernel'] = 'rbf'\n",
    "        \n",
    "        model = SVR(**current_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        preds = np.maximum(preds, 0)\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        \n",
    "        if mae < best_score:\n",
    "            best_score = mae\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Completed {i + 1}/{len(param_grid)} combinations...\")\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best MAE (Validation):\", best_score)\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Tune San Juan SVM\n",
    "print(\"=\" * 50)\n",
    "print(\"Tuning San Juan SVM Model...\")\n",
    "svm_model_sj, svm_params_sj = tune_svm(X_train_sj_scaled, y_train_sj, X_val_sj_scaled, y_val_sj)\n",
    "\n",
    "# Tune Iquitos SVM\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Tuning Iquitos SVM Model...\")\n",
    "svm_model_iq, svm_params_iq = tune_svm(X_train_iq_scaled, y_train_iq, X_val_iq_scaled, y_val_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL 3: 1D CNN =====\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds a 1D CNN model for time series regression.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Conv Block\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', \n",
    "               input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second Conv Block\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Third Conv Block\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Flatten and Dense layers\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mae',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_cnn(X_train, y_train, X_val, y_val, epochs=CNN_EPOCHS, batch_size=CNN_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a CNN model with early stopping.\n",
    "    For 1D CNN, we reshape data to (samples, timesteps, features).\n",
    "    Since we're treating each sample as a single timestep with multiple features,\n",
    "    we reshape to (samples, features, 1) - treating features as timesteps.\n",
    "    \"\"\"\n",
    "    # Reshape for 1D CNN: (samples, timesteps, channels)\n",
    "    # Here we treat each feature as a timestep\n",
    "    X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val_cnn = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    \n",
    "    input_shape = (X_train_cnn.shape[1], 1)\n",
    "    model = build_cnn_model(input_shape)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_cnn, y_train,\n",
    "        validation_data=(X_val_cnn, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_preds = model.predict(X_val_cnn, verbose=0).flatten()\n",
    "    val_preds = np.maximum(val_preds, 0)\n",
    "    val_mae = mean_absolute_error(y_val, val_preds)\n",
    "    print(f\"CNN Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train San Juan CNN\n",
    "print(\"=\" * 50)\n",
    "print(\"Training San Juan CNN Model...\")\n",
    "cnn_model_sj, cnn_history_sj = train_cnn(X_train_sj_scaled, y_train_sj.values, \n",
    "                                          X_val_sj_scaled, y_val_sj.values)\n",
    "\n",
    "# Train Iquitos CNN\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Iquitos CNN Model...\")\n",
    "cnn_model_iq, cnn_history_iq = train_cnn(X_train_iq_scaled, y_train_iq.values, \n",
    "                                          X_val_iq_scaled, y_val_iq.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70188b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GENERATE BASE MODEL PREDICTIONS FOR ENSEMBLE =====\n",
    "\n",
    "def get_base_predictions(xgb_model, svm_model, cnn_model, X_scaled, X_scaled_df):\n",
    "    \"\"\"\n",
    "    Generate predictions from all base models for stacking.\n",
    "    \"\"\"\n",
    "    # XGBoost predictions\n",
    "    xgb_preds = xgb_model.predict(X_scaled_df)\n",
    "    xgb_preds = np.maximum(xgb_preds, 0)\n",
    "    \n",
    "    # SVM predictions\n",
    "    svm_preds = svm_model.predict(X_scaled)\n",
    "    svm_preds = np.maximum(svm_preds, 0)\n",
    "    \n",
    "    # CNN predictions\n",
    "    X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
    "    cnn_preds = cnn_model.predict(X_cnn, verbose=0).flatten()\n",
    "    cnn_preds = np.maximum(cnn_preds, 0)\n",
    "    \n",
    "    # Stack predictions as features for meta-learner\n",
    "    stacked_features = np.column_stack([xgb_preds, svm_preds, cnn_preds])\n",
    "    \n",
    "    return stacked_features, xgb_preds, svm_preds, cnn_preds\n",
    "\n",
    "# Get predictions on validation set for training the meta-learner\n",
    "print(\"Generating San Juan stacked features...\")\n",
    "sj_val_stacked, sj_val_xgb, sj_val_svm, sj_val_cnn = get_base_predictions(\n",
    "    xgb_model_sj, svm_model_sj, cnn_model_sj, X_val_sj_scaled, X_val_sj_scaled_df\n",
    ")\n",
    "\n",
    "print(\"Generating Iquitos stacked features...\")\n",
    "iq_val_stacked, iq_val_xgb, iq_val_svm, iq_val_cnn = get_base_predictions(\n",
    "    xgb_model_iq, svm_model_iq, cnn_model_iq, X_val_iq_scaled, X_val_iq_scaled_df\n",
    ")\n",
    "\n",
    "# Get predictions on test set for final ensemble evaluation\n",
    "print(\"\\nGenerating San Juan test stacked features...\")\n",
    "sj_test_stacked, sj_test_xgb, sj_test_svm, sj_test_cnn = get_base_predictions(\n",
    "    xgb_model_sj, svm_model_sj, cnn_model_sj, X_test_sj_scaled, X_test_sj_scaled_df\n",
    ")\n",
    "\n",
    "print(\"Generating Iquitos test stacked features...\")\n",
    "iq_test_stacked, iq_test_xgb, iq_test_svm, iq_test_cnn = get_base_predictions(\n",
    "    xgb_model_iq, svm_model_iq, cnn_model_iq, X_test_iq_scaled, X_test_iq_scaled_df\n",
    ")\n",
    "\n",
    "print(\"\\nStacked features shapes:\")\n",
    "print(f\"SJ Val: {sj_val_stacked.shape}, SJ Test: {sj_test_stacked.shape}\")\n",
    "print(f\"IQ Val: {iq_val_stacked.shape}, IQ Test: {iq_test_stacked.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RANDOM FOREST META-LEARNER (ENSEMBLER) =====\n",
    "\n",
    "def tune_random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Tunes Random Forest meta-learner using grid search.\n",
    "    Training set = validation predictions from base models\n",
    "    Test set = test predictions from base models\n",
    "    \"\"\"\n",
    "    param_grid = list(ParameterGrid(RF_PARAM_GRID))\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"Tuning Random Forest on {len(param_grid)} combinations...\")\n",
    "    \n",
    "    for params in param_grid:\n",
    "        model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        preds = np.maximum(preds, 0)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        \n",
    "        if mae < best_score:\n",
    "            best_score = mae\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best MAE (Test):\", best_score)\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Train Random Forest meta-learner for San Juan\n",
    "print(\"=\" * 50)\n",
    "print(\"Training San Juan Random Forest Ensembler...\")\n",
    "rf_model_sj, rf_params_sj = tune_random_forest(sj_val_stacked, y_val_sj, sj_test_stacked, y_test_sj)\n",
    "\n",
    "# Train Random Forest meta-learner for Iquitos\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Iquitos Random Forest Ensembler...\")\n",
    "rf_model_iq, rf_params_iq = tune_random_forest(iq_val_stacked, y_val_iq, iq_test_stacked, y_test_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f910cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE EVALUATION AND COMPARISON =====\n",
    "\n",
    "# Evaluate individual models on test set\n",
    "print(\"=\" * 60)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# San Juan\n",
    "print(\"\\n--- San Juan ---\")\n",
    "mae_xgb_sj = mean_absolute_error(y_test_sj, sj_test_xgb)\n",
    "mae_svm_sj = mean_absolute_error(y_test_sj, sj_test_svm)\n",
    "mae_cnn_sj = mean_absolute_error(y_test_sj, sj_test_cnn)\n",
    "print(f\"XGBoost MAE: {mae_xgb_sj:.4f}\")\n",
    "print(f\"SVM MAE: {mae_svm_sj:.4f}\")\n",
    "print(f\"CNN MAE: {mae_cnn_sj:.4f}\")\n",
    "\n",
    "# Iquitos\n",
    "print(\"\\n--- Iquitos ---\")\n",
    "mae_xgb_iq = mean_absolute_error(y_test_iq, iq_test_xgb)\n",
    "mae_svm_iq = mean_absolute_error(y_test_iq, iq_test_svm)\n",
    "mae_cnn_iq = mean_absolute_error(y_test_iq, iq_test_cnn)\n",
    "print(f\"XGBoost MAE: {mae_xgb_iq:.4f}\")\n",
    "print(f\"SVM MAE: {mae_svm_iq:.4f}\")\n",
    "print(f\"CNN MAE: {mae_cnn_iq:.4f}\")\n",
    "\n",
    "# Ensemble predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE (Random Forest Meta-Learner)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# San Juan Ensemble\n",
    "ensemble_preds_sj = rf_model_sj.predict(sj_test_stacked)\n",
    "ensemble_preds_sj = np.maximum(ensemble_preds_sj, 0)\n",
    "mae_ensemble_sj = mean_absolute_error(y_test_sj, ensemble_preds_sj)\n",
    "print(f\"\\nSan Juan Ensemble MAE: {mae_ensemble_sj:.4f}\")\n",
    "\n",
    "# Iquitos Ensemble\n",
    "ensemble_preds_iq = rf_model_iq.predict(iq_test_stacked)\n",
    "ensemble_preds_iq = np.maximum(ensemble_preds_iq, 0)\n",
    "mae_ensemble_iq = mean_absolute_error(y_test_iq, ensemble_preds_iq)\n",
    "print(f\"Iquitos Ensemble MAE: {mae_ensemble_iq:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"San Juan - Best Individual: {min(mae_xgb_sj, mae_svm_sj, mae_cnn_sj):.4f}, Ensemble: {mae_ensemble_sj:.4f}\")\n",
    "print(f\"Iquitos  - Best Individual: {min(mae_xgb_iq, mae_svm_iq, mae_cnn_iq):.4f}, Ensemble: {mae_ensemble_iq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VISUALIZE ENSEMBLE PREDICTIONS VS ACTUAL =====\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# San Juan - All Models\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(y_test_sj.index, y_test_sj.values, label='Actual', linewidth=2, color='black')\n",
    "ax1.plot(y_test_sj.index, sj_test_xgb, label=f'XGBoost (MAE={mae_xgb_sj:.2f})', alpha=0.7)\n",
    "ax1.plot(y_test_sj.index, sj_test_svm, label=f'SVM (MAE={mae_svm_sj:.2f})', alpha=0.7)\n",
    "ax1.plot(y_test_sj.index, sj_test_cnn, label=f'CNN (MAE={mae_cnn_sj:.2f})', alpha=0.7)\n",
    "ax1.set_title('San Juan: Base Models vs Actual')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Total Cases')\n",
    "\n",
    "# San Juan - Ensemble\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(y_test_sj.index, y_test_sj.values, label='Actual', linewidth=2, color='black')\n",
    "ax2.plot(y_test_sj.index, ensemble_preds_sj, label=f'Ensemble (MAE={mae_ensemble_sj:.2f})', \n",
    "         linewidth=2, color='red')\n",
    "ax2.set_title('San Juan: Ensemble (RF) vs Actual')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Total Cases')\n",
    "\n",
    "# Iquitos - All Models\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(y_test_iq.index, y_test_iq.values, label='Actual', linewidth=2, color='black')\n",
    "ax3.plot(y_test_iq.index, iq_test_xgb, label=f'XGBoost (MAE={mae_xgb_iq:.2f})', alpha=0.7)\n",
    "ax3.plot(y_test_iq.index, iq_test_svm, label=f'SVM (MAE={mae_svm_iq:.2f})', alpha=0.7)\n",
    "ax3.plot(y_test_iq.index, iq_test_cnn, label=f'CNN (MAE={mae_cnn_iq:.2f})', alpha=0.7)\n",
    "ax3.set_title('Iquitos: Base Models vs Actual')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Total Cases')\n",
    "\n",
    "# Iquitos - Ensemble\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(y_test_iq.index, y_test_iq.values, label='Actual', linewidth=2, color='black')\n",
    "ax4.plot(y_test_iq.index, ensemble_preds_iq, label=f'Ensemble (MAE={mae_ensemble_iq:.2f})', \n",
    "         linewidth=2, color='red')\n",
    "ax4.set_title('Iquitos: Ensemble (RF) vs Actual')\n",
    "ax4.legend()\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Total Cases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a21ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RETRAIN ALL MODELS ON FULL TRAINING DATA =====\n",
    "\n",
    "# Combine train + val + test for final training\n",
    "# San Juan\n",
    "X_full_sj = pd.concat([X_train_sj, X_val_sj, X_test_sj])\n",
    "y_full_sj = pd.concat([y_train_sj, y_val_sj, y_test_sj])\n",
    "\n",
    "# Iquitos\n",
    "X_full_iq = pd.concat([X_train_iq, X_val_iq, X_test_iq])\n",
    "y_full_iq = pd.concat([y_train_iq, y_val_iq, y_test_iq])\n",
    "\n",
    "# Re-scale full data\n",
    "scaler_full_sj = StandardScaler()\n",
    "scaler_full_iq = StandardScaler()\n",
    "\n",
    "X_full_sj_scaled = scaler_full_sj.fit_transform(X_full_sj)\n",
    "X_full_sj_scaled_df = pd.DataFrame(X_full_sj_scaled, index=X_full_sj.index, columns=X_full_sj.columns)\n",
    "\n",
    "X_full_iq_scaled = scaler_full_iq.fit_transform(X_full_iq)\n",
    "X_full_iq_scaled_df = pd.DataFrame(X_full_iq_scaled, index=X_full_iq.index, columns=X_full_iq.columns)\n",
    "\n",
    "print(\"Retraining all base models on full data...\")\n",
    "\n",
    "# Retrain XGBoost\n",
    "print(\"\\n[1/3] Retraining XGBoost...\")\n",
    "final_xgb_sj = xgb.XGBRegressor(**xgb_model_sj.get_params())\n",
    "final_xgb_sj.fit(X_full_sj_scaled_df, y_full_sj, verbose=False)\n",
    "\n",
    "final_xgb_iq = xgb.XGBRegressor(**xgb_model_iq.get_params())\n",
    "final_xgb_iq.fit(X_full_iq_scaled_df, y_full_iq, verbose=False)\n",
    "\n",
    "# Retrain SVM\n",
    "print(\"[2/3] Retraining SVM...\")\n",
    "final_svm_sj = SVR(kernel='rbf', **svm_params_sj)\n",
    "final_svm_sj.fit(X_full_sj_scaled, y_full_sj)\n",
    "\n",
    "final_svm_iq = SVR(kernel='rbf', **svm_params_iq)\n",
    "final_svm_iq.fit(X_full_iq_scaled, y_full_iq)\n",
    "\n",
    "# Retrain CNN\n",
    "print(\"[3/3] Retraining CNN...\")\n",
    "X_full_sj_cnn = X_full_sj_scaled.reshape((X_full_sj_scaled.shape[0], X_full_sj_scaled.shape[1], 1))\n",
    "X_full_iq_cnn = X_full_iq_scaled.reshape((X_full_iq_scaled.shape[0], X_full_iq_scaled.shape[1], 1))\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(monitor='loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "final_cnn_sj = build_cnn_model((X_full_sj_cnn.shape[1], 1))\n",
    "final_cnn_sj.fit(X_full_sj_cnn, y_full_sj.values, epochs=CNN_EPOCHS, batch_size=CNN_BATCH_SIZE, \n",
    "                  callbacks=callbacks_final, verbose=0)\n",
    "\n",
    "final_cnn_iq = build_cnn_model((X_full_iq_cnn.shape[1], 1))\n",
    "final_cnn_iq.fit(X_full_iq_cnn, y_full_iq.values, epochs=CNN_EPOCHS, batch_size=CNN_BATCH_SIZE, \n",
    "                  callbacks=callbacks_final, verbose=0)\n",
    "\n",
    "print(\"\\nAll base models retrained on full data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c86423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FINAL PREDICTIONS ON TEST SET AND SUBMISSION =====\n",
    "\n",
    "# 1. Load Test Data\n",
    "test_features = pd.read_csv(os.path.join(DATA_DIR, 'dengue_features_test.csv'), index_col=[0, 1, 2])\n",
    "\n",
    "sj_test_features = test_features.loc['sj']\n",
    "iq_test_features = test_features.loc['iq']\n",
    "\n",
    "# 2. Preprocess Test Data\n",
    "sj_test = preprocess_city_data(sj_test_features)\n",
    "iq_test = preprocess_city_data(iq_test_features)\n",
    "\n",
    "# 3. Feature Engineering for Test Data (handling lags)\n",
    "def prepare_test_features(train_df, test_df, selected_features, rolling_weeks=ROLLING_WEEKS, \n",
    "                           lag_weeks=LAG_WEEKS, delta_weeks=DELTA_WEEKS):\n",
    "    \"\"\"\n",
    "    Combines train tail with test to generate features without NaNs in test.\n",
    "    \"\"\"\n",
    "    max_lookback = max(max(rolling_weeks), max(lag_weeks), max(delta_weeks)) + 50\n",
    "    train_subset = train_df[test_df.columns].tail(max_lookback)\n",
    "    combined = pd.concat([train_subset, test_df])\n",
    "    combined_eng = create_features(combined, lag_weeks=lag_weeks, rolling_weeks=rolling_weeks, \n",
    "                                   delta_weeks=delta_weeks)\n",
    "    combined_eng = combined_eng[selected_features]\n",
    "    test_eng = combined_eng.loc[test_df.index]\n",
    "    return test_eng\n",
    "\n",
    "print(\"Generating Test Features...\")\n",
    "sj_submission_test_eng = prepare_test_features(sj_train, sj_test, sj_features_selected)\n",
    "iq_submission_test_eng = prepare_test_features(iq_train, iq_test, iq_features_selected)\n",
    "\n",
    "# Align test features\n",
    "sj_features_kept = [c for c in X_full_sj.columns]\n",
    "iq_features_kept = [c for c in X_full_iq.columns]\n",
    "\n",
    "sj_submission_test_eng = sj_submission_test_eng[sj_features_kept]\n",
    "iq_submission_test_eng = iq_submission_test_eng[iq_features_kept]\n",
    "\n",
    "print(\"San Juan Test Shape:\", sj_submission_test_eng.shape)\n",
    "print(\"Iquitos Test Shape:\", iq_submission_test_eng.shape)\n",
    "\n",
    "# 4. Scale submission test data\n",
    "sj_submission_scaled = scaler_full_sj.transform(sj_submission_test_eng)\n",
    "sj_submission_scaled_df = pd.DataFrame(sj_submission_scaled, index=sj_submission_test_eng.index, \n",
    "                                        columns=sj_submission_test_eng.columns)\n",
    "\n",
    "iq_submission_scaled = scaler_full_iq.transform(iq_submission_test_eng)\n",
    "iq_submission_scaled_df = pd.DataFrame(iq_submission_scaled, index=iq_submission_test_eng.index, \n",
    "                                        columns=iq_submission_test_eng.columns)\n",
    "\n",
    "# 5. Generate base model predictions for submission\n",
    "print(\"\\nGenerating base model predictions for submission...\")\n",
    "\n",
    "# XGBoost predictions\n",
    "sj_sub_xgb = final_xgb_sj.predict(sj_submission_scaled_df)\n",
    "iq_sub_xgb = final_xgb_iq.predict(iq_submission_scaled_df)\n",
    "\n",
    "# SVM predictions\n",
    "sj_sub_svm = final_svm_sj.predict(sj_submission_scaled)\n",
    "iq_sub_svm = final_svm_iq.predict(iq_submission_scaled)\n",
    "\n",
    "# CNN predictions\n",
    "sj_sub_cnn_input = sj_submission_scaled.reshape((sj_submission_scaled.shape[0], sj_submission_scaled.shape[1], 1))\n",
    "iq_sub_cnn_input = iq_submission_scaled.reshape((iq_submission_scaled.shape[0], iq_submission_scaled.shape[1], 1))\n",
    "\n",
    "sj_sub_cnn = final_cnn_sj.predict(sj_sub_cnn_input, verbose=0).flatten()\n",
    "iq_sub_cnn = final_cnn_iq.predict(iq_sub_cnn_input, verbose=0).flatten()\n",
    "\n",
    "# 6. Stack predictions for ensemble\n",
    "sj_sub_stacked = np.column_stack([sj_sub_xgb, sj_sub_svm, sj_sub_cnn])\n",
    "iq_sub_stacked = np.column_stack([iq_sub_xgb, iq_sub_svm, iq_sub_cnn])\n",
    "\n",
    "# 7. Final ensemble prediction using Random Forest\n",
    "sj_final_preds = rf_model_sj.predict(sj_sub_stacked)\n",
    "sj_final_preds = np.maximum(sj_final_preds, 0).astype(int)\n",
    "\n",
    "iq_final_preds = rf_model_iq.predict(iq_sub_stacked)\n",
    "iq_final_preds = np.maximum(iq_final_preds, 0).astype(int)\n",
    "\n",
    "# 8. Create Submission\n",
    "submission = pd.read_csv(os.path.join(DATA_DIR, \"submission_format.csv\"), index_col=[0, 1, 2])\n",
    "\n",
    "# Assign values\n",
    "submission.loc['sj', 'total_cases'] = sj_final_preds\n",
    "submission.loc['iq', 'total_cases'] = iq_final_preds\n",
    "\n",
    "# 9. Save\n",
    "submission_path = os.path.join(RESULT_DIR, 'submission_ensemble_xgb_svm_cnn_rf.csv')\n",
    "submission.to_csv(submission_path)\n",
    "print(f\"\\nSubmission saved to {submission_path}\")\n",
    "print(f\"\\nPrediction Summary:\")\n",
    "print(f\"San Juan: min={sj_final_preds.min()}, max={sj_final_preds.max()}, mean={sj_final_preds.mean():.2f}\")\n",
    "print(f\"Iquitos: min={iq_final_preds.min()}, max={iq_final_preds.max()}, mean={iq_final_preds.mean():.2f}\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba589bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE MODEL SUMMARY =====\n",
    "print(\"=\" * 70)\n",
    "print(\"ENSEMBLE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                         INPUT FEATURES                               │\n",
    "│                    (Scaled with StandardScaler)                       │\n",
    "└───────────────────────────┬─────────────────────────────────────────┘\n",
    "                            │\n",
    "        ┌───────────────────┼───────────────────┐\n",
    "        │                   │                   │\n",
    "        ▼                   ▼                   ▼\n",
    "┌───────────────┐   ┌───────────────┐   ┌───────────────┐\n",
    "│    XGBoost    │   │      SVM      │   │    1D CNN     │\n",
    "│   Regressor   │   │    (SVR)      │   │   Regressor   │\n",
    "└───────┬───────┘   └───────┬───────┘   └───────┬───────┘\n",
    "        │                   │                   │\n",
    "        └───────────────────┼───────────────────┘\n",
    "                            │\n",
    "                            ▼\n",
    "              ┌─────────────────────────┐\n",
    "              │     STACKED FEATURES    │\n",
    "              │ [xgb_pred, svm_pred,    │\n",
    "              │  cnn_pred]              │\n",
    "              └───────────┬─────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "              ┌─────────────────────────┐\n",
    "              │    Random Forest        │\n",
    "              │    (Meta-Learner)       │\n",
    "              └───────────┬─────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "              ┌─────────────────────────┐\n",
    "              │   FINAL PREDICTION      │\n",
    "              │   (total_cases)         │\n",
    "              └─────────────────────────┘\n",
    "\"\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  XGBoost SJ: {xgb_params_sj}\")\n",
    "print(f\"  XGBoost IQ: {xgb_params_iq}\")\n",
    "print(f\"  SVM SJ: {svm_params_sj}\")\n",
    "print(f\"  SVM IQ: {svm_params_iq}\")\n",
    "print(f\"  RF Ensembler SJ: {rf_params_sj}\")\n",
    "print(f\"  RF Ensembler IQ: {rf_params_iq}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
