{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8017a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM Model for Dengue Prediction\n",
    "# This notebook implements an LSTM (Long Short-Term Memory) neural network for the Dengue prediction task.\n",
    "\n",
    "# ## Approach\n",
    "# 1. **Preprocessing**: Standard cleaning and forward filling.\n",
    "# 2. **Feature Engineering**: Rolling mean and standard deviation features are added. Explicit lag features are omitted as the LSTM sequence input handles temporal history.\n",
    "# 3. **Scaling**: StandardScaler\n",
    "# 4. **Sequencing**: Data is transformed into sequences of length `SEQ_LEN` (e.g., 20 weeks) to feed into the LSTM.\n",
    "# 5. **Model**: A stacked LSTM architecture with Dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "WORKING_DIR = 'C:/term_project'\n",
    "os.chdir(WORKING_DIR)\n",
    "DATA_DIR = os.path.join(WORKING_DIR, 'data')\n",
    "RESULT_DIR = os.path.join(WORKING_DIR, 'results')\n",
    "\n",
    "print(\"Libraries imported and environment set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb762a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "LAG_WEEKS = [4, 8, 12]\n",
    "ROLLING_WEEKS = [4, 8, 12]\n",
    "PEARSON_PRUNE_THRESHOLD = 0.95\n",
    "SEQ_LEN = 20  # Number of weeks in each input sequence\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.2\n",
    "PATIENCE = 10  # Early stopping patience\n",
    "\n",
    "# LSTM Parameter Grid\n",
    "PARAM_GRID = {\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [16, 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "train_features = pd.read_csv(os.path.join(DATA_DIR, 'dengue_features_train.csv'), index_col=[0, 1, 2])\n",
    "train_labels = pd.read_csv(os.path.join(DATA_DIR, 'dengue_labels_train.csv'), index_col=[0, 1, 2])\n",
    "\n",
    "# Seperate data for San Juan\n",
    "sj_train_features = train_features.loc['sj']\n",
    "sj_train_labels = train_labels.loc['sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features.loc['iq']\n",
    "iq_train_labels = train_labels.loc['iq']\n",
    "\n",
    "print(\"San Juan features:\", sj_train_features.shape)\n",
    "print(\"Iquitos features:\", iq_train_features.shape)\n",
    "\n",
    "# PREPROCESS DATA \n",
    "\n",
    "def preprocess_city_data(features, labels=None):\n",
    "    \"\"\"\n",
    "    Merges features and labels, converts week_start_date to index,\n",
    "    and handles missing values.\n",
    "    \"\"\"\n",
    "    # 1. Merge features and labels if labels are provided\n",
    "    if labels is not None:\n",
    "        df = features.join(labels)\n",
    "    else:\n",
    "        df = features.copy()\n",
    "    \n",
    "    # 2. Reset index to move 'year' and 'weekofyear' from index to columns\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # 3. Convert week_start_date to datetime\n",
    "    df['week_start_date'] = pd.to_datetime(df['week_start_date'])\n",
    "    \n",
    "    # 4. Set week_start_date as index\n",
    "    df.set_index('week_start_date', inplace=True)\n",
    "    \n",
    "    # 5. Drop 'city' column\n",
    "    if 'city' in df.columns:\n",
    "        df.drop(columns=['city'], inplace=True)\n",
    "    \n",
    "    # 6. Fill Missing Values (Forward Fill)\n",
    "    df = df.ffill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to San Juan and Iquitos\n",
    "sj_train = preprocess_city_data(sj_train_features, sj_train_labels)\n",
    "iq_train = preprocess_city_data(iq_train_features, iq_train_labels)\n",
    "\n",
    "print(\"San Juan Preprocessed:\", sj_train.shape)\n",
    "print(\"Iquitos Preprocessed:\", iq_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "\n",
    "def add_engineered_features(df, rolling_weeks=ROLLING_WEEKS, lag_weeks=LAG_WEEKS):\n",
    "    \"\"\"\n",
    "    Adds rolling mean/std and lag features.\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    # Select numeric feature columns (exclude target and time identifiers if needed)\n",
    "    # We keep year/weekofyear as features as they capture seasonality\n",
    "    feature_cols = [c for c in df.columns if c not in ['total_cases']]\n",
    "    \n",
    "    new_features = []\n",
    "    for col in feature_cols:\n",
    "        # Skip year/weekofyear for rolling stats if desired, but they are monotonic/cyclic so maybe skip\n",
    "        if col in ['year', 'weekofyear']:\n",
    "            continue\n",
    "            \n",
    "        # Rolling features\n",
    "        for window in rolling_weeks:\n",
    "            rolling = df_eng[col].rolling(window=window)\n",
    "            new_features.append(rolling.mean().rename(f'{col}_mean_{window}'))\n",
    "            new_features.append(rolling.std().rename(f'{col}_std_{window}'))\n",
    "            \n",
    "        # Lag features\n",
    "        for lag in lag_weeks:\n",
    "            new_features.append(df_eng[col].shift(lag).rename(f'{col}_lag_{lag}'))\n",
    "    \n",
    "    if new_features:\n",
    "        df_features = pd.concat(new_features, axis=1)\n",
    "        df_eng = pd.concat([df_eng, df_features], axis=1)\n",
    "        \n",
    "    # Drop NaNs created by rolling/lags\n",
    "    df_eng.dropna(inplace=True)\n",
    "    return df_eng\n",
    "\n",
    "def remove_collinear_features(df, threshold):\n",
    "    \"\"\"\n",
    "    Removes features that are highly correlated with each other.\n",
    "    Keeps the first feature and drops the subsequent highly correlated ones.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    # Only use feature columns (exclude target)\n",
    "    if 'total_cases' in df_clean.columns:\n",
    "        features = df_clean.drop(columns=['total_cases'])\n",
    "    else:\n",
    "        features = df_clean\n",
    "        \n",
    "    corr_matrix = features.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    print(f\"Dropping {len(to_drop)} features due to collinearity > {threshold}\")\n",
    "    df_clean.drop(columns=to_drop, inplace=True)\n",
    "    \n",
    "    return df_clean, to_drop\n",
    "\n",
    "print(\"Adding engineered features (rolling + lags)...\")\n",
    "sj_train_eng = add_engineered_features(sj_train)\n",
    "iq_train_eng = add_engineered_features(iq_train)\n",
    "\n",
    "print(\"Removing collinear features...\")\n",
    "sj_train_eng, sj_dropped = remove_collinear_features(sj_train_eng, PEARSON_PRUNE_THRESHOLD)\n",
    "iq_train_eng, iq_dropped = remove_collinear_features(iq_train_eng, PEARSON_PRUNE_THRESHOLD)\n",
    "\n",
    "# Store kept features for test set consistency\n",
    "sj_features_kept = [c for c in sj_train_eng.columns if c != 'total_cases']\n",
    "iq_features_kept = [c for c in iq_train_eng.columns if c != 'total_cases']\n",
    "\n",
    "print(\"San Juan Engineered:\", sj_train_eng.shape)\n",
    "print(\"Iquitos Engineered:\", iq_train_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING AND SEQUENCE GENERATION\n",
    "\n",
    "def create_sequences(data, target, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        x = data[i:(i + seq_len)]\n",
    "        y = target[i + seq_len]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def prepare_data_for_lstm(df, target_col='total_cases', seq_len=SEQ_LEN, scaler_X=None, scaler_y=None, is_train=True):\n",
    "    # Separate features and target\n",
    "    if target_col in df.columns:\n",
    "        X = df.drop(columns=[target_col]).values\n",
    "        y = df[[target_col]].values\n",
    "    else:\n",
    "        X = df.values\n",
    "        y = None\n",
    "        \n",
    "    # Scale X\n",
    "    if is_train:\n",
    "        scaler_X = StandardScaler()\n",
    "        X_scaled = scaler_X.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = scaler_X.transform(X)\n",
    "        \n",
    "    # Scale y\n",
    "    if y is not None:\n",
    "        if is_train:\n",
    "            scaler_y = StandardScaler()\n",
    "            y_scaled = scaler_y.fit_transform(y)\n",
    "        else:\n",
    "            y_scaled = scaler_y.transform(y)\n",
    "    else:\n",
    "        y_scaled = None\n",
    "        \n",
    "    # Create sequences\n",
    "    if is_train:\n",
    "        X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_len)\n",
    "        return X_seq, y_seq, scaler_X, scaler_y\n",
    "    else:\n",
    "        # For test/inference, we assume the input df already has the necessary history prepended\n",
    "        # We just slide the window to create sequences for each target point\n",
    "        # If input has length N, and we want to predict for the last M points where N = M + SEQ_LEN\n",
    "        # We generate M sequences.\n",
    "        # Actually, let's just generate all possible sequences from the input\n",
    "        xs = []\n",
    "        # We start at 0. The first sequence is 0:SEQ_LEN.\n",
    "        # The last sequence is len-SEQ_LEN:len.\n",
    "        for i in range(len(X_scaled) - seq_len + 1):\n",
    "             xs.append(X_scaled[i:(i + seq_len)])\n",
    "        return np.array(xs), None, scaler_X, scaler_y\n",
    "\n",
    "# Prepare San Juan Data\n",
    "# We need to split into train/val BEFORE scaling to avoid leakage? \n",
    "# Or split after? Standard practice is split then scale.\n",
    "# But for time series, we can just split the dataframe first.\n",
    "\n",
    "def split_and_prepare(df, split_ratio=0.8):\n",
    "    split_idx = int(len(df) * split_ratio)\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    val_df = df.iloc[split_idx:]\n",
    "    \n",
    "    # Prepare Train\n",
    "    X_train, y_train, scaler_X, scaler_y = prepare_data_for_lstm(train_df, is_train=True)\n",
    "    \n",
    "    # Prepare Val\n",
    "    # We need to prepend the last SEQ_LEN rows of train to val to predict the first val point\n",
    "    # Otherwise we lose the first SEQ_LEN points of validation\n",
    "    train_tail = train_df.iloc[-SEQ_LEN:]\n",
    "    val_df_extended = pd.concat([train_tail, val_df])\n",
    "    \n",
    "    # Note: We pass the fitted scalers\n",
    "    X_val, y_val, _, _ = prepare_data_for_lstm(val_df_extended, scaler_X=scaler_X, scaler_y=scaler_y, is_train=False)\n",
    "    \n",
    "    # However, prepare_data_for_lstm(is_train=False) returns all sequences.\n",
    "    # If val_df_extended has length L + V, it returns (L+V - L + 1) = V + 1 sequences?\n",
    "    # Wait. \n",
    "    # If len is 20 (SEQ_LEN) + 1. Range is (21 - 20 + 1) = 2. i=0, i=1.\n",
    "    # i=0: 0:20. i=1: 1:21.\n",
    "    # We only want V sequences corresponding to the V points in val_df.\n",
    "    # The targets for val_df are val_df['total_cases'].\n",
    "    # We need to extract y_val manually or adjust the function.\n",
    "    \n",
    "    # Let's adjust:\n",
    "    # Get y_val from val_df directly and scale it\n",
    "    y_val_raw = val_df[['total_cases']].values\n",
    "    y_val_scaled = scaler_y.transform(y_val_raw)\n",
    "    \n",
    "    # X_val sequences:\n",
    "    # We want exactly len(val_df) sequences.\n",
    "    # The first sequence should end just before the first val point? No.\n",
    "    # To predict y_val[0], we need X[0-SEQ_LEN : 0] (relative to val start).\n",
    "    # So we need exactly SEQ_LEN history.\n",
    "    # If val_df_extended = train_tail(SEQ_LEN) + val_df(V).\n",
    "    # Total len = SEQ_LEN + V.\n",
    "    # We want V sequences.\n",
    "    # i=0: take 0:SEQ_LEN. This predicts the point at index SEQ_LEN (which is val_df[0]).\n",
    "    # Last i: len - SEQ_LEN.\n",
    "    # Range(len - SEQ_LEN) goes from 0 to V-1. Total V iterations.\n",
    "    # So create_sequences logic works if we just ignore the y output from it and use our own y.\n",
    "    \n",
    "    # Let's use a simpler loop for X_val\n",
    "    X_val_extended = val_df_extended.drop(columns=['total_cases']).values\n",
    "    X_val_scaled = scaler_X.transform(X_val_extended)\n",
    "    \n",
    "    xs_val = []\n",
    "    for i in range(len(val_df)):\n",
    "        # Sequence for val point i starts at i in the extended array\n",
    "        # and has length SEQ_LEN.\n",
    "        # Extended array: [TrainTail(SEQ_LEN) | Val(V)]\n",
    "        # i=0: slice 0:SEQ_LEN. Correct.\n",
    "        xs_val.append(X_val_scaled[i : i + SEQ_LEN])\n",
    "        \n",
    "    X_val = np.array(xs_val)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val_scaled, scaler_X, scaler_y, val_df\n",
    "\n",
    "print(\"Preparing Data...\")\n",
    "# Use 1 - TEST_SPLIT_RATIO for training split\n",
    "train_ratio = 1.0 - TEST_SPLIT_RATIO\n",
    "X_train_sj, y_train_sj, X_val_sj, y_val_sj, scaler_X_sj, scaler_y_sj, val_df_sj = split_and_prepare(sj_train_eng, split_ratio=train_ratio)\n",
    "X_train_iq, y_train_iq, X_val_iq, y_val_iq, scaler_X_iq, scaler_y_iq, val_df_iq = split_and_prepare(iq_train_eng, split_ratio=train_ratio)\n",
    "\n",
    "print(\"San Juan Train:\", X_train_sj.shape, y_train_sj.shape)\n",
    "print(\"San Juan Val:\", X_val_sj.shape, y_val_sj.shape)\n",
    "print(\"Iquitos Train:\", X_train_iq.shape, y_train_iq.shape)\n",
    "print(\"Iquitos Val:\", X_val_iq.shape, y_val_iq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80591b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING WITH GRID SEARCH\n",
    "\n",
    "def build_lstm_model(input_shape, units=64, dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units // 2, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val, params):\n",
    "    model = build_lstm_model(\n",
    "        (X_train.shape[1], X_train.shape[2]), \n",
    "        units=params['units'], \n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        learning_rate=params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0 # Silent during grid search\n",
    "    )\n",
    "    \n",
    "    # Return the best validation loss achieved during training\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss, model\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*PARAM_GRID.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "print(f\"Grid Search with {len(param_combinations)} combinations...\")\n",
    "\n",
    "def grid_search(X_train, y_train, X_val, y_val, city_name):\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"\\nStarting Grid Search for {city_name}...\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"[{i+1}/{len(param_combinations)}] Testing params: {params}\", end='\\r')\n",
    "        try:\n",
    "            val_loss, model = train_and_evaluate(X_train, y_train, X_val, y_val, params)\n",
    "            \n",
    "            if val_loss < best_score:\n",
    "                best_score = val_loss\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError with params {params}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"\\nBest {city_name} Params: {best_params}\")\n",
    "    print(f\"Best {city_name} Val Loss (MAE): {best_score:.4f}\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Run Grid Search for San Juan\n",
    "model_sj, best_params_sj = grid_search(X_train_sj, y_train_sj, X_val_sj, y_val_sj, \"San Juan\")\n",
    "\n",
    "# Run Grid Search for Iquitos\n",
    "model_iq, best_params_iq = grid_search(X_train_iq, y_train_iq, X_val_iq, y_val_iq, \"Iquitos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "def evaluate_model(model, X_val, y_val_scaled, scaler_y, city_name):\n",
    "    # Predict\n",
    "    preds_scaled = model.predict(X_val)\n",
    "    \n",
    "    # Inverse transform\n",
    "    preds = scaler_y.inverse_transform(preds_scaled)\n",
    "    y_true = scaler_y.inverse_transform(y_val_scaled)\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    preds = np.maximum(preds, 0)\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    print(f\"{city_name} MAE: {mae:.4f}\")\n",
    "    \n",
    "    return y_true, preds\n",
    "\n",
    "# Evaluate San Juan\n",
    "y_true_sj, preds_sj = evaluate_model(model_sj, X_val_sj, y_val_sj, scaler_y_sj, \"San Juan\")\n",
    "\n",
    "# Evaluate Iquitos\n",
    "y_true_iq, preds_iq = evaluate_model(model_iq, X_val_iq, y_val_iq, scaler_y_iq, \"Iquitos\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# San Juan\n",
    "ax[0].plot(val_df_sj.index, y_true_sj, label='Actual')\n",
    "ax[0].plot(val_df_sj.index, preds_sj, label='Predicted')\n",
    "ax[0].set_title('San Juan: Actual vs Predicted (LSTM)')\n",
    "ax[0].legend()\n",
    "\n",
    "# Iquitos\n",
    "ax[1].plot(val_df_iq.index, y_true_iq, label='Actual')\n",
    "ax[1].plot(val_df_iq.index, preds_iq, label='Predicted')\n",
    "ax[1].set_title('Iquitos: Actual vs Predicted (LSTM)')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d065fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMISSION\n",
    "\n",
    "# 1. Load Test Data\n",
    "test_features = pd.read_csv(os.path.join(DATA_DIR, 'dengue_features_test.csv'), index_col=[0, 1, 2])\n",
    "sj_test_features = test_features.loc['sj']\n",
    "iq_test_features = test_features.loc['iq']\n",
    "\n",
    "# 2. Preprocess Test Data\n",
    "sj_test = preprocess_city_data(sj_test_features)\n",
    "iq_test = preprocess_city_data(iq_test_features)\n",
    "\n",
    "def prepare_test_sequences(train_df, test_df, scaler_X, features_kept, seq_len=SEQ_LEN, rolling_weeks=ROLLING_WEEKS, lag_weeks=LAG_WEEKS):\n",
    "    # 1. Combine Train Tail + Test to handle Rolling Features\n",
    "    # We need enough history for the largest rolling window or lag\n",
    "    max_rolling = max(rolling_weeks) if rolling_weeks else 0\n",
    "    max_lag = max(lag_weeks) if lag_weeks else 0\n",
    "    max_history = max(max_rolling, max_lag)\n",
    "    \n",
    "    # Add a buffer\n",
    "    buffer = max_history + seq_len + 10\n",
    "    \n",
    "    train_subset = train_df.tail(buffer)\n",
    "    \n",
    "    # Ensure columns match (train might have extra columns if we didn't filter)\n",
    "    # train_df has 'total_cases', test_df doesn't.\n",
    "    # We only need feature columns.\n",
    "    feature_cols = [c for c in train_df.columns if c != 'total_cases']\n",
    "    \n",
    "    # Note: train_df here is the original preprocessed df (sj_train), not the engineered one.\n",
    "    # So it doesn't have rolling features yet.\n",
    "    \n",
    "    train_subset = train_subset[feature_cols]\n",
    "    test_subset = test_df[feature_cols] # Should match\n",
    "    \n",
    "    combined = pd.concat([train_subset, test_subset])\n",
    "    \n",
    "    # 2. Feature Engineering\n",
    "    combined_eng = add_engineered_features(combined, rolling_weeks=rolling_weeks, lag_weeks=lag_weeks)\n",
    "    \n",
    "    # 3. Filter Features (Pruning)\n",
    "    # We must select exactly the same features as in training\n",
    "    # features_kept contains the list of columns from training\n",
    "    combined_eng = combined_eng[features_kept]\n",
    "    \n",
    "    # 4. Prepare for LSTM\n",
    "    # We need to extract the part corresponding to Test, PLUS seq_len history\n",
    "    # The test part starts after the train_subset part.\n",
    "    # But wait, add_engineered_features drops NaNs.\n",
    "    # The NaNs would be at the beginning of combined.\n",
    "    # Since we added a buffer, the \"test\" part is safe.\n",
    "    # We need to find where the test data starts.\n",
    "    # We can use the index.\n",
    "    \n",
    "    # Identify test indices\n",
    "    test_indices = test_df.index\n",
    "    \n",
    "    # We need the data from (start_of_test_index - seq_len) to end.\n",
    "    # Let's find the integer location of the first test row in combined_eng\n",
    "    # It's safer to just take the last (len(test_df) + seq_len) rows?\n",
    "    # Yes, assuming combined_eng covers it.\n",
    "    # We added buffer = max_history + seq_len + 10.\n",
    "    # Rolling/Lags drops max_history rows (approx).\n",
    "    # So we have plenty.\n",
    "    \n",
    "    required_len = len(test_df) + seq_len\n",
    "    if len(combined_eng) < required_len:\n",
    "        raise ValueError(f\"Not enough history! Have {len(combined_eng)}, need {required_len}\")\n",
    "        \n",
    "    data_for_seq = combined_eng.iloc[-required_len:]\n",
    "    \n",
    "    # 5. Scale\n",
    "    X_scaled = scaler_X.transform(data_for_seq.values)\n",
    "    \n",
    "    # 6. Create Sequences\n",
    "    # We want exactly len(test_df) sequences.\n",
    "    xs = []\n",
    "    for i in range(len(test_df)):\n",
    "        # i=0: we want the sequence ending just before test_df[0]? \n",
    "        # No, to predict test_df[0], we need input X[0:SEQ_LEN] from data_for_seq.\n",
    "        # data_for_seq has length SEQ_LEN + M.\n",
    "        # i=0: slice 0:SEQ_LEN.\n",
    "        xs.append(X_scaled[i : i + seq_len])\n",
    "        \n",
    "    return np.array(xs)\n",
    "\n",
    "print(\"Generating Test Sequences...\")\n",
    "X_test_sj = prepare_test_sequences(sj_train, sj_test, scaler_X_sj, sj_features_kept, rolling_weeks=ROLLING_WEEKS, lag_weeks=LAG_WEEKS)\n",
    "X_test_iq = prepare_test_sequences(iq_train, iq_test, scaler_X_iq, iq_features_kept, rolling_weeks=ROLLING_WEEKS, lag_weeks=LAG_WEEKS)\n",
    "\n",
    "print(\"San Juan Test Input:\", X_test_sj.shape)\n",
    "print(\"Iquitos Test Input:\", X_test_iq.shape)\n",
    "\n",
    "# Predict\n",
    "sj_preds_scaled = model_sj.predict(X_test_sj)\n",
    "iq_preds_scaled = model_iq.predict(X_test_iq)\n",
    "\n",
    "# Inverse Transform\n",
    "sj_preds_final = scaler_y_sj.inverse_transform(sj_preds_scaled)\n",
    "iq_preds_final = scaler_y_iq.inverse_transform(iq_preds_scaled)\n",
    "\n",
    "# Clip and Round\n",
    "sj_preds_final = np.maximum(sj_preds_final, 0).astype(int).flatten()\n",
    "iq_preds_final = np.maximum(iq_preds_final, 0).astype(int).flatten()\n",
    "\n",
    "# Create Submission\n",
    "submission = pd.read_csv(os.path.join(DATA_DIR, \"submission_format.csv\"), index_col=[0, 1, 2])\n",
    "submission.loc['sj', 'total_cases'] = sj_preds_final\n",
    "submission.loc['iq', 'total_cases'] = iq_preds_final\n",
    "\n",
    "submission_path = os.path.join(WORKING_DIR, 'results', 'submission_lstm_2.csv')\n",
    "submission.to_csv(submission_path)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dca609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
